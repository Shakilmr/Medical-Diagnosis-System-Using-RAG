{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf(\"/Users/shakil/Downloads/RAG-Chatbot/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "# Assume extracted_data is defined somewhere\n",
    "text_chunks = text_split(extracted_data)  # Call the function to get text chunks\n",
    "\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "# Call the function and assign the result to a global variable\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "\n",
    "# Now you can use the embeddings object\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length:\", len(query_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pinecode Index using Code\n",
    "from pinecone import Pinecone, ServerlessSpec  # Import the required classes  \n",
    "# Initialize Pinecone with an instance \n",
    "pc = Pinecone(api_key=\"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\")  \n",
    "# Replace with your actual API key  \n",
    "# Define index name \n",
    "index_name = \"medibot\"  \n",
    "# Create the index if it doesn't exist if index_name not in pc.list_indexes():    \n",
    "pc.create_index(\n",
    "\tname=index_name,         \n",
    "\tdimension=384,         \n",
    "\tmetric=\"cosine\",         \n",
    "\tspec=ServerlessSpec\n",
    "\t(             \n",
    "\t\tcloud=\"aws\",             \n",
    "\t\tregion=\"us-east-1\"         \n",
    "\t)     \n",
    ")\n",
    "#Using the same code we created the medicalbot index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your Pinecone API key directly as a string\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\"\n",
    "\n",
    "# Verify that the environment variable is set correctly\n",
    "print(os.environ[\"PINECONE_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Updated import\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "# Step 1: Split text into chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)  \n",
    "print(\"Length of Text Chunks:\", len(text_chunks))\n",
    "\n",
    "# Step 2: Download Hugging Face embeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length of Embedding Vector:\", len(query_result))\n",
    "\n",
    "# Step 3: Upload vector embeddings to Pinecone\n",
    "docsearch = Pinecone.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"medibot\"\n",
    ")\n",
    "\n",
    "print(\"Vector embeddings uploaded to Pinecone successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Set your Pinecone API key and environment\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\"\n",
    "os.environ[\"PINECONE_ENVIRONMENT\"] = \"us-east-1\"  # Optional\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Specify your existing index name\n",
    "index_name = \"testbot\"\n",
    "\n",
    "# Load the existing Pinecone index\n",
    "vector_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"Existing Pinecone index loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is diabetes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Specify your existing index name\n",
    "index_name = \"testbot\"\n",
    "\n",
    "# Load the existing Pinecone index\n",
    "vector_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Define the strict prompt template\n",
    "def create_strict_prompt_template(context, question):\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"You must ONLY use information from the provided context to answer the question. \"\n",
    "        \"Do not include any external knowledge or information not present in the context. \"\n",
    "        \"If the context doesn't contain enough information to answer the question, \"\n",
    "        \"respond with 'I cannot answer this question based on the provided context.' \"\n",
    "        \"Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question: {input}\\n\\n\"\n",
    "        \"Remember: Only use information from the context above. \"\n",
    "        \"If the answer isn't in the context, say you cannot answer.\"\n",
    "    )\n",
    "    return system_prompt.format(context=context, input=question)\n",
    "\n",
    "# Function to format retrieved documents into a single context string\n",
    "def format_retrieved_context(retrieved_docs):\n",
    "    formatted_texts = []\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        content = doc.page_content if hasattr(doc, 'page_content') else doc.content\n",
    "        metadata = doc.metadata if hasattr(doc, 'metadata') else {}\n",
    "        metadata_str = f\" (Source: {metadata.get('source', 'Unknown')})\" if metadata else \"\"\n",
    "        formatted_texts.append(\n",
    "            f\"[Document {i}{metadata_str}]\\n{content.strip()}\\n[End Document {i}]\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted_texts)\n",
    "\n",
    "# Function to answer a question using GPT4All and the retrieved context\n",
    "def answer_question(question, retriever):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    \n",
    "    # Format context from the retrieved documents\n",
    "    formatted_context = format_retrieved_context(retrieved_docs)\n",
    "    \n",
    "    # Create the prompt with strict instructions\n",
    "    final_prompt = create_strict_prompt_template(formatted_context, question)\n",
    "    \n",
    "    # Initialize GPT4All with the specified GGUF model\n",
    "    model_name = \"dolphin-2.2.1-mistral-7b.Q4_K_M.gguf\"  # filename of your model\n",
    "    model_path = \"/Users/shakil/Downloads/RAG-Chatbot/model/\"  # directory containing your model\n",
    "    # Using 'gpu' here to leverage Apple Silicon's Metal backend on ARM64\n",
    "    llm = GPT4All(\n",
    "        model_name=model_name,\n",
    "        model_path=model_path,\n",
    "        model_type=\"gguf\",\n",
    "        device='gpu'\n",
    "    )\n",
    "    \n",
    "    # Generate the answer\n",
    "    with llm.chat_session():\n",
    "        answer = llm.generate(final_prompt, max_tokens=150)\n",
    "    \n",
    "    return answer, retrieved_docs\n",
    "\n",
    "def main():\n",
    "    question = \"What is headache?\"\n",
    "    answer, retrieved_docs = answer_question(question, retriever)\n",
    "    \n",
    "    # Format the answer so that after every full stop there is a new line.\n",
    "    # You can use either simple replacement or a regex for robustness.\n",
    "    formatted_answer = re.sub(r'\\.\\s+', '.\\n', answer)\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"Answer:\")\n",
    "    print(formatted_answer)\n",
    "    print(\"\\nRetrieved Documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(doc.page_content if hasattr(doc, 'page_content') else doc.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables before importing Pinecone-related modules\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\"\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"us-east-1\"\n",
    "\n",
    "import re\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Use updated import to remove deprecation warning\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Specify your existing index name\n",
    "index_name = \"testbot\"\n",
    "\n",
    "# Load the existing Pinecone index\n",
    "vector_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store (returning top 3 similar docs)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# --- Prompt Template & Helper Functions ---\n",
    "def create_strict_prompt_template(context, question):\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"You must ONLY use information from the provided context to answer the question. \"\n",
    "        \"Do not include any external knowledge or information not present in the context. \"\n",
    "        \"If the context doesn't contain enough information to answer the question, \"\n",
    "        \"respond with 'I cannot answer this question based on the provided context.' \"\n",
    "        \"Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question: {input}\\n\\n\"\n",
    "        \"Remember: Only use information from the context above. \"\n",
    "        \"If the answer isn't in the context, say you cannot answer.\"\n",
    "    )\n",
    "    return system_prompt.format(context=context, input=question)\n",
    "\n",
    "def format_retrieved_context(retrieved_docs):\n",
    "    formatted_texts = []\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        content = doc.page_content if hasattr(doc, 'page_content') else doc.content\n",
    "        metadata = doc.metadata if hasattr(doc, 'metadata') else {}\n",
    "        metadata_str = f\" (Source: {metadata.get('source', 'Unknown')})\" if metadata else \"\"\n",
    "        formatted_texts.append(\n",
    "            f\"[Document {i}{metadata_str}]\\n{content.strip()}\\n[End Document {i}]\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted_texts)\n",
    "\n",
    "def answer_question(question):\n",
    "    # Retrieve relevant documents from the Pinecone index\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_retrieved_context(retrieved_docs)\n",
    "    final_prompt = create_strict_prompt_template(formatted_context, question)\n",
    "    \n",
    "    # Initialize GPT4All using your model configuration\n",
    "    model_name = \"dolphin-2.2.1-mistral-7b.Q4_K_M.gguf\"  # Your model file\n",
    "    model_path = \"/Users/shakil/Downloads/RAG-Chatbot/model/\"  # Directory containing your model\n",
    "    llm = GPT4All(\n",
    "        model_name=model_name,\n",
    "        model_path=model_path,\n",
    "        model_type=\"gguf\",\n",
    "        device=\"gpu\"  # Use 'gpu' to leverage Metal on Apple Silicon (or 'cpu' if needed)\n",
    "    )\n",
    "    \n",
    "    # Generate the answer within a chat session\n",
    "    with llm.chat_session():\n",
    "        answer = llm.generate(final_prompt, max_tokens=150)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# --- Flask Routes ---\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    # Render the chat interface page (templates/index.html)\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    question = data.get(\"question\", \"\")\n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"No question provided\"}), 400\n",
    "    answer = answer_question(question)\n",
    "    # Format the answer: insert a newline after each period\n",
    "    formatted_answer = re.sub(r'\\.\\s+', '.\\n', answer)\n",
    "    return jsonify({\"answer\": formatted_answer})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8080, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the repository ID and filename\n",
    "repo_id = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\"\n",
    "filename = \"DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\"\n",
    "\n",
    "# Define the target directory\n",
    "target_dir = \"/Users/shakil/Downloads/RAG-Chatbot/model\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Download the model file\n",
    "model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "# Move the downloaded file to the target directory\n",
    "shutil.move(model_path, os.path.join(target_dir, filename))\n",
    "\n",
    "print(f\"Model downloaded and moved to: {os.path.join(target_dir, filename)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf(\"/Users/shakil/Downloads/RAG-Chatbot/data\") \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Updated import\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "# Step 1: Split text into chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)  \n",
    "print(\"Length of Text Chunks:\", len(text_chunks))\n",
    "\n",
    "# Step 2: Download Hugging Face embeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length of Embedding Vector:\", len(query_result))\n",
    "\n",
    "# Step 3: Upload vector embeddings to Pinecone\n",
    "docsearch = Pinecone.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"medicalbot\"\n",
    ")\n",
    "\n",
    "print(\"Medical book Vector embeddings uploaded to Pinecone successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final 1\n",
    "import os\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from gpt4all import GPT4All\n",
    "import re\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\"\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"us-east-1\"\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"medicalbot\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize LLM only once\n",
    "llm = GPT4All(\n",
    "    model_name=\"Qwen2-1.5B-Instruct.Q8_0.gguf\",\n",
    "    model_path=\"/Users/shakil/Downloads/RAG-Chatbot/model/\",\n",
    "    device=\"gpu\",\n",
    "    n_ctx=512\n",
    ")\n",
    "\n",
    "def format_retrieved_context(retrieved_docs):\n",
    "    \"\"\"Format documents with metadata\"\"\"\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"• {doc.page_content} (Source: {doc.metadata.get('source', 'Unknown')})\"\n",
    "        for doc in retrieved_docs\n",
    "    ])\n",
    "\n",
    "def create_medical_prompt(context, question, include_price):\n",
    "    \"\"\"Generate structured prompt with conditional pricing\"\"\"\n",
    "    price_instruction = \"Include price in BDT format if available. \" if include_price else \"\"\n",
    "    \n",
    "    return f\"\"\"You are a medical assistant. Use ONLY the provided context. \n",
    "    \n",
    "    IMPORTANT: Your response must be exactly 3-4 sentences total. Be concise and direct.\n",
    "    \n",
    "    {price_instruction}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Response (3-4 sentences only):\"\"\"\n",
    "\n",
    "def create_general_medical_prompt(question):\n",
    "    \"\"\"Create a prompt for general medical information\"\"\"\n",
    "    return f\"\"\"You are a medical expert. Provide a concise, medically accurate answer.\n",
    "    \n",
    "    IMPORTANT: Your response must be exactly 3-4 sentences total. Be concise and direct.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer (3-4 sentences only):\"\"\"\n",
    "\n",
    "def count_sentences(text):\n",
    "    \"\"\"Count the number of sentences in text\"\"\"\n",
    "    # Basic sentence splitting by period, exclamation mark, or question mark\n",
    "    sentence_count = len(re.findall(r'[.!?]+', text))\n",
    "    return sentence_count\n",
    "\n",
    "def ensure_sentence_limit(text, max_sentences=4):\n",
    "    \"\"\"Ensure the text has no more than the specified number of sentences\"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r'([.!?])\\s+', text)\n",
    "    \n",
    "    # Group sentences back with their punctuation\n",
    "    complete_sentences = []\n",
    "    i = 0\n",
    "    sentence_count = 0\n",
    "    \n",
    "    while i < len(sentences) and sentence_count < max_sentences:\n",
    "        if i + 1 < len(sentences) and sentences[i+1] in ['.', '!', '?']:\n",
    "            complete_sentences.append(sentences[i] + sentences[i+1])\n",
    "            i += 2\n",
    "            sentence_count += 1\n",
    "        else:\n",
    "            # For sentences without ending punctuation\n",
    "            if sentences[i] and not sentences[i] in ['.', '!', '?']:\n",
    "                # Add period if there's actual content and it's not already punctuation\n",
    "                if not sentences[i].strip().endswith(('.', '!', '?')):\n",
    "                    complete_sentences.append(sentences[i] + '.')\n",
    "                else:\n",
    "                    complete_sentences.append(sentences[i])\n",
    "                sentence_count += 1\n",
    "            i += 1\n",
    "    \n",
    "    # Join sentences back together\n",
    "    result = ' '.join(complete_sentences)\n",
    "    return result\n",
    "\n",
    "def answer_question(question):\n",
    "    try:\n",
    "        # Detect if it's a price-related question\n",
    "        has_price = any(word in question.lower() for word in ['price', 'cost', 'how much', 'fee'])\n",
    "        \n",
    "        # Detect if the question is a general disease inquiry\n",
    "        disease_keywords = ['what is', 'explain', 'symptoms of', 'causes of', 'treatment for']\n",
    "        is_general_medical_query = any(keyword in question.lower() for keyword in disease_keywords)\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        docs_with_scores = vector_store.similarity_search_with_score(question, k=3)\n",
    "        filtered_docs = [doc for doc, score in docs_with_scores if score >= 0.6]\n",
    "        \n",
    "        if not filtered_docs:\n",
    "            # If it's a general disease question, fetch a generic medical response\n",
    "            if is_general_medical_query:\n",
    "                # Use the same LLM for general medical information\n",
    "                general_prompt = create_general_medical_prompt(question)\n",
    "                with llm.chat_session():\n",
    "                    response = llm.generate(general_prompt, max_tokens=100, temp=0.5)\n",
    "                response = ensure_sentence_limit(response.strip())\n",
    "                return response\n",
    "            else:\n",
    "                return \"I cannot answer this question based on the available information.\"\n",
    "            \n",
    "        # Generate prompt with context\n",
    "        formatted_context = format_retrieved_context(filtered_docs)\n",
    "        prompt = create_medical_prompt(formatted_context, question, has_price)\n",
    "\n",
    "        # Generate response (using the same LLM instance)\n",
    "        with llm.chat_session():\n",
    "            # Lower max_tokens to encourage brevity\n",
    "            response = llm.generate(prompt, max_tokens=100, temp=0.5)\n",
    "        \n",
    "        # Ensure sentence limit\n",
    "        response = ensure_sentence_limit(response.strip())\n",
    "        \n",
    "        # Post-process pricing if needed\n",
    "        if has_price:\n",
    "            response = re.sub(r'(\\d+)(\\s*BDT)', r'\\1.00 BDT', response)\n",
    "            response = re.sub(r'(?<=\\d)(?=[A-Za-z])', ' ', response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\"\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    question = data.get(\"question\", \"\")\n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"No question provided\"}), 400\n",
    "    \n",
    "    answer = answer_question(question)\n",
    "    return jsonify({\"answer\": answer})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8080, debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pinecode Index using Code\n",
    "from pinecone import Pinecone, ServerlessSpec  # Import the required classes  \n",
    "# Initialize Pinecone with an instance \n",
    "pc = Pinecone(api_key=\"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\")  \n",
    "# Replace with your actual API key  \n",
    "# Define index name \n",
    "index_name = \"medicalbot\"  \n",
    "# Create the index if it doesn't exist if index_name not in pc.list_indexes():    \n",
    "pc.create_index(\n",
    "\tname=index_name,         \n",
    "\tdimension=384,         \n",
    "\tmetric=\"cosine\",         \n",
    "\tspec=ServerlessSpec\n",
    "\t(             \n",
    "\t\tcloud=\"aws\",             \n",
    "\t\tregion=\"us-east-1\"         \n",
    "\t)     \n",
    ")\n",
    "#Using the same code we created the medicalbot index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Updated import\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "# --- Step 1: Split extracted PDF data into numbered rows ---\n",
    "\n",
    "def split_into_numbered_rows(documents):\n",
    "    \"\"\"\n",
    "    Splits each document's text into rows based on a pattern where a row begins with a number.\n",
    "    Each row is returned as a new Document with a 'row_number' in metadata.\n",
    "    Rows are sorted in ascending order by their row number.\n",
    "    \"\"\"\n",
    "    numbered_docs = []\n",
    "    for doc in documents:\n",
    "        # Use regex to split at positions where a new line starts with one or more digits and a space.\n",
    "        # (?m) enables multiline mode and (?=^\\d+\\s) is a positive lookahead for a line beginning with digits.\n",
    "        rows = re.split(r'(?m)(?=^\\d+\\s)', doc.page_content)\n",
    "        rows = [row.strip() for row in rows if row.strip()]\n",
    "        row_tuples = []\n",
    "        for row in rows:\n",
    "            # Match the row number and the rest of the row\n",
    "            m = re.match(r'^(\\d+)\\s+(.*)', row, flags=re.DOTALL)\n",
    "            if m:\n",
    "                row_number = int(m.group(1))\n",
    "                row_text = row.strip()\n",
    "                row_tuples.append((row_number, row_text))\n",
    "        # Sort rows by the extracted row number\n",
    "        row_tuples.sort(key=lambda x: x[0])\n",
    "        for number, text in row_tuples:\n",
    "            new_doc = Document(page_content=text, metadata={\"row_number\": number})\n",
    "            numbered_docs.append(new_doc)\n",
    "    return numbered_docs\n",
    "\n",
    "# Example: assuming 'extracted_data' is a list of Document objects loaded from \"Price 2.pdf\"\n",
    "# For instance, extracted_data could be obtained via a PDF loader.\n",
    "extracted_data = load_pdf(\"/Users/shakil/Downloads/RAG-Chatbot/price/\")\n",
    "\n",
    "numbered_documents = split_into_numbered_rows(extracted_data)\n",
    "print(\"Number of numbered rows:\", len(numbered_documents))\n",
    "\n",
    "# --- Step 2: Download Hugging Face embeddings ---\n",
    "\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length of Embedding Vector:\", len(query_result))\n",
    "\n",
    "# --- Step 3: Upload vector embeddings to Pinecone ---\n",
    "\n",
    "docsearch = Pinecone.from_documents(\n",
    "    documents=numbered_documents,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"test\"  # Ensure this index exists in your Pinecone account\n",
    ")\n",
    "\n",
    "print(\"Price Vector embeddings uploaded to Pinecone successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from gpt4all import GPT4All\n",
    "import re\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\"\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"us-east-1\"\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"medibot\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize LLM only once\n",
    "llm = GPT4All(\n",
    "    model_name=\"Qwen2-1.5B-Instruct.Q8_0.gguf\",\n",
    "    model_path=\"/Users/shakil/Downloads/RAG-Chatbot/model/\",\n",
    "    device=\"gpu\",\n",
    "    n_ctx=512\n",
    ")\n",
    "\n",
    "# Bot identity information\n",
    "BOT_INFO = {\n",
    "    \"name\": \"MediAssist\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"description\": \"A professional healthcare assistant providing accurate medical information and pricing.\",\n",
    "    \"capabilities\": [\n",
    "        \"Answering medical questions based on verified information\",\n",
    "        \"Providing treatment pricing information\",\n",
    "        \"Offering general healthcare guidance\"\n",
    "    ],\n",
    "    \"limitations\": [\n",
    "        \"Cannot provide personalized medical diagnosis\",\n",
    "        \"Information is limited to the available knowledge database\",\n",
    "        \"Not a replacement for professional medical consultation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def is_about_bot(question):\n",
    "    \"\"\"Detect if the question is asking about the bot itself\"\"\"\n",
    "    bot_keywords = [\n",
    "        \"who are you\", \"what are you\", \"your name\", \"about you\", \"tell me about yourself\",\n",
    "        \"what can you do\", \"your capabilities\", \"how do you work\", \"who made you\", \n",
    "        \"what is this\", \"chatbot\", \"medibot\", \"assistant\", \"your purpose\", \"your function\"\n",
    "    ]\n",
    "    \n",
    "    question_lower = question.lower()\n",
    "    return any(keyword in question_lower for keyword in bot_keywords)\n",
    "\n",
    "def get_bot_response():\n",
    "    \"\"\"Generate a professional response about the bot\"\"\"\n",
    "    return f\"I am {BOT_INFO['name']}, a healthcare information assistant (version {BOT_INFO['version']}). I provide evidence-based medical information and pricing details from my knowledge database. While I can offer general healthcare information, I'm not a substitute for professional medical advice, and my responses are limited to information in my database.\"\n",
    "\n",
    "def extract_price_info(docs):\n",
    "    \"\"\"Extract price information from document metadata and content\"\"\"\n",
    "    price_data = {}\n",
    "    \n",
    "    for doc in docs:\n",
    "        # First check if metadata has price information\n",
    "        if doc.metadata.get('has_price_info') and 'price_data' in doc.metadata:\n",
    "            price_data.update(doc.metadata['price_data'])\n",
    "        \n",
    "        # Also scan the content for price patterns\n",
    "        content = doc.page_content\n",
    "        price_matches = re.findall(r'([A-Za-z\\s\\-,/\\(\\)]+)\\s+(\\d+(?:\\.\\d+)?)\\s*(?:BDT|Tk\\.?)?', content)\n",
    "        \n",
    "        for treatment, price in price_matches:\n",
    "            treatment = treatment.strip()\n",
    "            if treatment and not treatment.isdigit():  # Avoid false positives\n",
    "                price_data[treatment] = price\n",
    "    \n",
    "    return price_data\n",
    "\n",
    "def format_retrieved_context(retrieved_docs):\n",
    "    \"\"\"Format documents with metadata and highlighted price information\"\"\"\n",
    "    # Extract price information from all documents\n",
    "    price_info = extract_price_info(retrieved_docs)\n",
    "    \n",
    "    # Format document content for context\n",
    "    context_parts = []\n",
    "    \n",
    "    for doc in retrieved_docs:\n",
    "        content = doc.page_content\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        \n",
    "        # Add document type information if available\n",
    "        doc_type = \"\"\n",
    "        if doc.metadata.get('document_type'):\n",
    "            doc_type = f\" ({doc.metadata.get('document_type')})\"\n",
    "        \n",
    "        # Add content type information if available\n",
    "        content_type = \"\"\n",
    "        if doc.metadata.get('content_type'):\n",
    "            content_type = f\" - {doc.metadata.get('content_type')}\"\n",
    "        \n",
    "        # Create formatted context entry\n",
    "        context_parts.append(f\"• {content} (Source: {source}{doc_type}{content_type})\")\n",
    "    \n",
    "    # If price information is available, add it as a separate section\n",
    "    if price_info:\n",
    "        price_entries = []\n",
    "        for treatment, price in price_info.items():\n",
    "            price_entries.append(f\"• {treatment}: {price} BDT\")\n",
    "        \n",
    "        context_parts.append(\"\\nPrice Information:\\n\" + \"\\n\".join(price_entries))\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "def create_medical_prompt(context, question, include_price):\n",
    "    \"\"\"Generate structured prompt with conditional pricing\"\"\"\n",
    "    price_instruction = \"Include price in BDT format if available. \" if include_price else \"\"\n",
    "    \n",
    "    return f\"\"\"You are a professional medical assistant named MediAssist. Use ONLY the provided context to answer the question.\n",
    "    If the information is not in the context, say \"I don't have information about that in my database. Please consult with a healthcare professional for personalized advice.\"\n",
    "    \n",
    "    IMPORTANT: Your response must be exactly 3-4 sentences total. Be concise, direct, and professional.\n",
    "    \n",
    "    {price_instruction}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Response (3-4 sentences only, maintain professional tone):\"\"\"\n",
    "\n",
    "def create_price_prompt(context, question):\n",
    "    \"\"\"Create a specialized prompt for price inquiries\"\"\"\n",
    "    return f\"\"\"You are MediAssist, a professional medical pricing assistant. Use ONLY the provided context to answer the question.\n",
    "    If the pricing information is not in the context, say \"I don't have pricing information about that in my database. Please contact the relevant healthcare facility directly for current pricing.\"\n",
    "    \n",
    "    IMPORTANT: Your response must be exactly 3-4 sentences total. Be concise, direct, and professional.\n",
    "    When mentioning prices, always use the format: X.00 BDT.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Response (3-4 sentences only, maintain professional tone):\"\"\"\n",
    "\n",
    "def count_sentences(text):\n",
    "    \"\"\"Count the number of sentences in text\"\"\"\n",
    "    # Basic sentence splitting by period, exclamation mark, or question mark\n",
    "    sentence_count = len(re.findall(r'[.!?]+', text))\n",
    "    return sentence_count\n",
    "\n",
    "def ensure_sentence_limit(text, max_sentences=4):\n",
    "    \"\"\"Ensure the text has no more than the specified number of sentences\"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r'([.!?])\\s+', text)\n",
    "    \n",
    "    # Group sentences back with their punctuation\n",
    "    complete_sentences = []\n",
    "    i = 0\n",
    "    sentence_count = 0\n",
    "    \n",
    "    while i < len(sentences) and sentence_count < max_sentences:\n",
    "        if i + 1 < len(sentences) and sentences[i+1] in ['.', '!', '?']:\n",
    "            complete_sentences.append(sentences[i] + sentences[i+1])\n",
    "            i += 2\n",
    "            sentence_count += 1\n",
    "        else:\n",
    "            # For sentences without ending punctuation\n",
    "            if sentences[i] and not sentences[i] in ['.', '!', '?']:\n",
    "                # Add period if there's actual content and it's not already punctuation\n",
    "                if not sentences[i].strip().endswith(('.', '!', '?')):\n",
    "                    complete_sentences.append(sentences[i] + '.')\n",
    "                else:\n",
    "                    complete_sentences.append(sentences[i])\n",
    "                sentence_count += 1\n",
    "            i += 1\n",
    "    \n",
    "    # Join sentences back together\n",
    "    result = ' '.join(complete_sentences)\n",
    "    return result\n",
    "\n",
    "def format_price_response(response, price_info):\n",
    "    \"\"\"Format the response to include price information consistently\"\"\"\n",
    "    # Make sure prices are in consistent format\n",
    "    response = re.sub(r'(\\d+)(?:\\.00)?\\s*(?:BDT|Tk\\.?)', r'\\1.00 BDT', response)\n",
    "    \n",
    "    # Ensure treatment names are separated from prices\n",
    "    response = re.sub(r'(?<=\\S)(?=\\d+\\.00 BDT)', ' ', response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def answer_question(question):\n",
    "    try:\n",
    "        # Check if question is about the bot itself\n",
    "        if is_about_bot(question):\n",
    "            return get_bot_response()\n",
    "        \n",
    "        # Detect if it's a price-related question\n",
    "        has_price = any(word in question.lower() for word in ['price', 'cost', 'how much', 'fee', 'charge', 'payment'])\n",
    "        \n",
    "        # Determine optimal search parameters\n",
    "        search_k = 5 if has_price else 3  # Retrieve more documents for price questions\n",
    "        similarity_threshold = 0.55 if has_price else 0.6  # Lower threshold for price queries\n",
    "        \n",
    "        # Add price-related terms to query to improve retrieval\n",
    "        search_query = question\n",
    "        if has_price and not any(term in search_query.lower() for term in ['price', 'cost']):\n",
    "            search_query += \" price cost\"\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        docs_with_scores = vector_store.similarity_search_with_score(search_query, k=search_k)\n",
    "        filtered_docs = [doc for doc, score in docs_with_scores if score >= similarity_threshold]\n",
    "        \n",
    "        # Extract price information from all retrieved documents\n",
    "        price_info = {}\n",
    "        if has_price:\n",
    "            price_info = extract_price_info(filtered_docs)\n",
    "        \n",
    "        if not filtered_docs:\n",
    "            return \"I don't have information about that in my database. Please consult with a healthcare professional for more specific guidance.\"\n",
    "            \n",
    "        # Generate prompt with context\n",
    "        formatted_context = format_retrieved_context(filtered_docs)\n",
    "        \n",
    "        # Choose appropriate prompt based on query type\n",
    "        if has_price:\n",
    "            prompt = create_price_prompt(formatted_context, question)\n",
    "        else:\n",
    "            prompt = create_medical_prompt(formatted_context, question, has_price)\n",
    "\n",
    "        # Generate response (using the same LLM instance)\n",
    "        with llm.chat_session():\n",
    "            # Lower max_tokens to encourage brevity\n",
    "            response = llm.generate(prompt, max_tokens=150, temp=0.5)\n",
    "        \n",
    "        # Ensure sentence limit\n",
    "        response = ensure_sentence_limit(response.strip())\n",
    "        \n",
    "        # Post-process pricing if needed\n",
    "        if has_price:\n",
    "            response = format_price_response(response, price_info)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"I apologize, but I encountered a technical issue while processing your request. Please try again with a different question or contact support if the problem persists.\"\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    question = data.get(\"question\", \"\")\n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"No question provided\"}), 400\n",
    "    \n",
    "    answer = answer_question(question)\n",
    "    return jsonify({\"answer\": answer})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8080, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final 2\n",
    "import os\n",
    "import re\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from gpt4all import GPT4All\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\"\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"us-east-1\"\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create two separate vector stores:\n",
    "# \"test\" index is used for direct treatment price queries (ending with BDT)\n",
    "vector_store_test = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"test\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "# \"medicalbot\" index is used for general queries\n",
    "vector_store_medicalbot = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"medicalbot\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize LLM only once\n",
    "llm = GPT4All(\n",
    "    model_name=\"Qwen2-1.5B-Instruct.Q8_0.gguf\",\n",
    "    model_path=\"/Users/shakil/Downloads/RAG-Chatbot/model/\",\n",
    "    device=\"gpu\",\n",
    "    n_ctx=1024\n",
    ")\n",
    "\n",
    "# Bot identity information\n",
    "BOT_INFO = {\n",
    "    \"name\": \"MediAssist\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"description\": \"A professional healthcare assistant providing accurate medical information and pricing.\",\n",
    "    \"capabilities\": [\n",
    "        \"Answering medical questions based on verified information\",\n",
    "        \"Providing treatment pricing information\",\n",
    "        \"Offering general healthcare guidance\"\n",
    "    ],\n",
    "    \"limitations\": [\n",
    "        \"Cannot provide personalized medical diagnosis\",\n",
    "        \"Information is limited to the available knowledge database\",\n",
    "        \"Not a replacement for professional medical consultation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def is_about_bot(question):\n",
    "    \"\"\"Detect if the question is asking about the bot itself\"\"\"\n",
    "    bot_keywords = [\n",
    "        \"who are you\", \"what are you\", \"your name\", \"about you\", \"tell me about yourself\",\n",
    "        \"what can you do\", \"your capabilities\", \"how do you work\", \"who made you\",\n",
    "        \"what is this\", \"chatbot\", \"medibot\", \"assistant\", \"your purpose\", \"your function\"\n",
    "    ]\n",
    "    return any(keyword in question.lower() for keyword in bot_keywords)\n",
    "\n",
    "def get_bot_response():\n",
    "    \"\"\"Generate a professional response about the bot\"\"\"\n",
    "    return (f\"I am {BOT_INFO['name']}, a healthcare information assistant (version {BOT_INFO['version']}). \"\n",
    "            \"I provide evidence-based medical information and pricing details from my knowledge database. \"\n",
    "            \"While I can offer general healthcare information, I'm not a substitute for professional medical advice, \"\n",
    "            \"and my responses are limited to information in my database.\")\n",
    "\n",
    "def extract_price_info(docs):\n",
    "    \"\"\"Extract price information from document metadata and content\"\"\"\n",
    "    price_data = {}\n",
    "    for doc in docs:\n",
    "        # Check metadata if available\n",
    "        if doc.metadata.get('has_price_info') and 'price_data' in doc.metadata:\n",
    "            price_data.update(doc.metadata['price_data'])\n",
    "        # Also scan the content for price patterns\n",
    "        content = doc.page_content\n",
    "        price_matches = re.findall(r'([A-Za-z\\s\\-,/\\(\\)]+)\\s+(\\d+(?:\\.\\d+)?)\\s*(?:BDT|Tk\\.?)', content)\n",
    "        for treatment, price in price_matches:\n",
    "            treatment = treatment.strip()\n",
    "            if treatment and not treatment.isdigit():\n",
    "                price_data[treatment.lower()] = price  # store keys in lowercase\n",
    "    return price_data\n",
    "\n",
    "def extract_price_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract treatment name and price from a text string.\n",
    "    Expected format: \"<row_number> <treatment> BDT <price>\"\n",
    "    For example: \"57 HbA1c BDT 1100\"\n",
    "    \"\"\"\n",
    "    match = re.search(r'^\\d+\\s+([A-Za-z0-9\\s\\-,/\\(\\)]+?)\\s+BDT\\s+(\\d+(?:\\.\\d+)?)', text, flags=re.I)\n",
    "    if match:\n",
    "        treatment = match.group(1).strip().lower()\n",
    "        price = match.group(2).strip()\n",
    "        return treatment, price\n",
    "    return None, None\n",
    "\n",
    "def format_retrieved_context(retrieved_docs):\n",
    "    \"\"\"Format documents with metadata and highlighted price information\"\"\"\n",
    "    price_info = extract_price_info(retrieved_docs)\n",
    "    context_parts = []\n",
    "    for doc in retrieved_docs:\n",
    "        content = doc.page_content\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        doc_type = f\" ({doc.metadata.get('document_type')})\" if doc.metadata.get('document_type') else \"\"\n",
    "        content_type = f\" - {doc.metadata.get('content_type')}\" if doc.metadata.get('content_type') else \"\"\n",
    "        context_parts.append(f\"• {content} (Source: {source}{doc_type}{content_type})\")\n",
    "    if price_info:\n",
    "        price_entries = [f\"• {treatment.title()}: {price} BDT\" for treatment, price in price_info.items()]\n",
    "        context_parts.append(\"\\nPrice Information:\\n\" + \"\\n\".join(price_entries))\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "def create_medical_prompt(context, question, include_price):\n",
    "    \"\"\"Generate structured prompt with conditional pricing\"\"\"\n",
    "    price_instruction = \"Include price in BDT format if available. \" if include_price else \"\"\n",
    "    return f\"\"\"You are a professional medical assistant named MediAssist. Use ONLY the provided context to answer the question.\n",
    "If the information is not in the context, say \"I don't have information about that in my database. Please consult with a healthcare professional for personalized advice.\"\n",
    "\n",
    "IMPORTANT: Your response must be exactly 3-4 sentences total. Be concise, direct, and professional.\n",
    "\n",
    "{price_instruction}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response (3-4 sentences only, maintain professional tone):\"\"\"\n",
    "\n",
    "def create_price_prompt(context, question):\n",
    "    \"\"\"Create a specialized prompt for price inquiries\"\"\"\n",
    "    return f\"\"\"You are MediAssist, a professional medical pricing assistant. Use ONLY the provided context to answer the question.\n",
    "If the pricing information is not in the context, say \"I don't have pricing information about that in my database. Please contact the relevant healthcare facility directly for current pricing.\"\n",
    "\n",
    "IMPORTANT: Your response must be exactly 3-4 sentences total. Be concise, direct, and professional.\n",
    "When mentioning prices, always use the format: X.00 BDT.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response (3-4 sentences only, maintain professional tone):\"\"\"\n",
    "\n",
    "def count_sentences(text):\n",
    "    \"\"\"Count the number of sentences in text\"\"\"\n",
    "    sentence_count = len(re.findall(r'[.!?]+', text))\n",
    "    return sentence_count\n",
    "\n",
    "def ensure_sentence_limit(text, max_sentences=4):\n",
    "    \"\"\"Ensure the text has no more than the specified number of sentences\"\"\"\n",
    "    sentences = re.split(r'([.!?])\\s+', text)\n",
    "    complete_sentences = []\n",
    "    i = 0\n",
    "    sentence_count = 0\n",
    "    while i < len(sentences) and sentence_count < max_sentences:\n",
    "        if i + 1 < len(sentences) and sentences[i+1] in ['.', '!', '?']:\n",
    "            complete_sentences.append(sentences[i] + sentences[i+1])\n",
    "            i += 2\n",
    "            sentence_count += 1\n",
    "        else:\n",
    "            if sentences[i] and sentences[i] not in ['.', '!', '?']:\n",
    "                if not sentences[i].strip().endswith(('.', '!', '?')):\n",
    "                    complete_sentences.append(sentences[i] + '.')\n",
    "                else:\n",
    "                    complete_sentences.append(sentences[i])\n",
    "                sentence_count += 1\n",
    "            i += 1\n",
    "    return ' '.join(complete_sentences)\n",
    "\n",
    "def format_price_response(response, price_info):\n",
    "    \"\"\"Format the response to include price information consistently\"\"\"\n",
    "    response = re.sub(r'(\\d+)(?:\\.00)?\\s*(?:BDT|Tk\\.?)', r'\\1.00 BDT', response)\n",
    "    response = re.sub(r'(?<=\\S)(?=\\d+\\.00 BDT)', ' ', response)\n",
    "    return response\n",
    "\n",
    "def answer_question(question):\n",
    "    try:\n",
    "        # If question exactly ends with \"BDT\", use the test index for direct price lookup.\n",
    "        m = re.match(r'^\\s*(.+?)\\s*BDT\\s*$', question, flags=re.I)\n",
    "        if m:\n",
    "            treatment_query = m.group(1).strip().lower()\n",
    "            # Use test index for treatment cost lookup\n",
    "            docs_with_scores = vector_store_test.similarity_search_with_score(treatment_query, k=20)\n",
    "            filtered_docs = [doc for doc, score in docs_with_scores if score >= 0.3]\n",
    "            for doc in filtered_docs:\n",
    "                treatment, price = extract_price_from_text(doc.page_content)\n",
    "                if treatment and treatment_query in treatment:\n",
    "                    return f\"The cost for {treatment.upper()} is {price}.00 BDT\"\n",
    "            return \"I don't have pricing information about that in my database. Please contact the relevant healthcare facility directly for current pricing.\"\n",
    "        \n",
    "        # For other queries, use the medicalbot index.\n",
    "        if is_about_bot(question):\n",
    "            return get_bot_response()\n",
    "        \n",
    "        has_price = any(word in question.lower() for word in ['price', 'cost', 'how much', 'fee', 'charge', 'payment'])\n",
    "        search_k = 5 if has_price else 3\n",
    "        similarity_threshold = 0.55 if has_price else 0.6\n",
    "        \n",
    "        search_query = question\n",
    "        if has_price and not any(term in search_query.lower() for term in ['price', 'cost']):\n",
    "            search_query += \" price cost\"\n",
    "        docs_with_scores = vector_store_medicalbot.similarity_search_with_score(search_query, k=search_k)\n",
    "        filtered_docs = [doc for doc, score in docs_with_scores if score >= similarity_threshold]\n",
    "        \n",
    "        price_info = {}\n",
    "        if has_price:\n",
    "            price_info = extract_price_info(filtered_docs)\n",
    "        if not filtered_docs:\n",
    "            return \"I don't have information about that in my database. Please consult with a healthcare professional for more specific guidance.\"\n",
    "        \n",
    "        formatted_context = format_retrieved_context(filtered_docs)\n",
    "        if has_price:\n",
    "            prompt = create_price_prompt(formatted_context, question)\n",
    "        else:\n",
    "            prompt = create_medical_prompt(formatted_context, question, has_price)\n",
    "        with llm.chat_session():\n",
    "            response = llm.generate(prompt, max_tokens=150, temp=0.5)\n",
    "        response = ensure_sentence_limit(response.strip())\n",
    "        if has_price:\n",
    "            response = format_price_response(response, price_info)\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"I apologize, but I encountered a technical issue while processing your request. Please try again with a different question or contact support if the problem persists.\"\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    question = data.get(\"question\", \"\")\n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"No question provided\"}), 400\n",
    "    answer = answer_question(question)\n",
    "    return jsonify({\"answer\": answer})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8080, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final 3\n",
    "import os\n",
    "import re\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from gpt4all import GPT4All\n",
    "import functools\n",
    "import concurrent.futures\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\"\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"us-east-1\"\n",
    "\n",
    "# Initialize embeddings - load once at startup\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector stores - load once at startup\n",
    "vector_store_test = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"test\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "vector_store_medicalbot = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"medicalbot\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize LLM with optimized settings\n",
    "llm = GPT4All(\n",
    "    model_name=\"Qwen2-1.5B-Instruct.Q8_0.gguf\",\n",
    "    model_path=\"/Users/shakil/Downloads/RAG-Chatbot/model/\",\n",
    "    device=\"gpu\",\n",
    "    n_ctx=1024,\n",
    "    n_threads=4  # Adjust based on your CPU\n",
    ")\n",
    "\n",
    "# Pre-compile frequently used regex patterns\n",
    "PRICE_PATTERN = re.compile(r'([A-Za-z\\s\\-,/\\(\\)]+)\\s+(\\d+(?:\\.\\d+)?)\\s*(?:BDT|Tk\\.?)')\n",
    "TREATMENT_PRICE_PATTERN = re.compile(r'^\\d+\\s+([A-Za-z0-9\\s\\-,/\\(\\)]+?)\\s+BDT\\s+(\\d+(?:\\.\\d+)?)', flags=re.I)\n",
    "SENTENCE_PATTERN = re.compile(r'[.!?]+')\n",
    "PRICE_FORMAT_PATTERN = re.compile(r'(\\d+)(?:\\.00)?\\s*(?:BDT|Tk\\.?)')\n",
    "SPACE_BEFORE_PRICE_PATTERN = re.compile(r'(?<=\\S)(?=\\d+\\.00 BDT)')\n",
    "BDT_QUERY_PATTERN = re.compile(r'^\\s*(.+?)\\s*BDT\\s*$', flags=re.I)\n",
    "GENERAL_PRICE_PATTERN = re.compile(r'BDT\\s+(\\d+(?:\\.\\d+)?)')\n",
    "\n",
    "# Bot identity information\n",
    "BOT_INFO = {\n",
    "    \"name\": \"MediAssist\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"description\": \"A professional healthcare assistant providing accurate medical information and pricing.\",\n",
    "    \"capabilities\": [\n",
    "        \"Answering medical questions based on verified information\",\n",
    "        \"Providing treatment pricing information\",\n",
    "        \"Offering general healthcare guidance\"\n",
    "    ],\n",
    "    \"limitations\": [\n",
    "        \"Cannot provide personalized medical diagnosis\",\n",
    "        \"Information is limited to the available knowledge database\",\n",
    "        \"Not a replacement for professional medical consultation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Precomputed bot keywords for faster matching\n",
    "BOT_KEYWORDS = frozenset([\n",
    "    \"who are you\", \"what are you\", \"your name\", \"about you\", \"tell me about yourself\",\n",
    "    \"what can you do\", \"your capabilities\", \"how do you work\", \"who made you\",\n",
    "    \"what is this\", \"chatbot\", \"medibot\", \"assistant\", \"your purpose\", \"your function\"\n",
    "])\n",
    "\n",
    "# Precomputed price keywords for faster matching\n",
    "PRICE_KEYWORDS = frozenset(['price', 'cost', 'how much', 'fee', 'charge', 'payment'])\n",
    "\n",
    "@functools.lru_cache(maxsize=128)\n",
    "def normalize_treatment_name(name):\n",
    "    \"\"\"Normalize treatment name for better matching (cached for performance)\"\"\"\n",
    "    return re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "\n",
    "@functools.lru_cache(maxsize=256)\n",
    "def treatments_match(query, treatment):\n",
    "    \"\"\"Check if treatment names match (cached for performance)\"\"\"\n",
    "    query_norm = normalize_treatment_name(query)\n",
    "    treatment_norm = normalize_treatment_name(treatment)\n",
    "    \n",
    "    # Direct match\n",
    "    if query_norm == treatment_norm:\n",
    "        return True\n",
    "    \n",
    "    # Check if one is a substring of the other\n",
    "    if query_norm in treatment_norm or treatment_norm in query_norm:\n",
    "        return True\n",
    "    \n",
    "    # Check if most words match\n",
    "    query_words = set(query_norm.split())\n",
    "    treatment_words = set(treatment_norm.split())\n",
    "    common_words = query_words.intersection(treatment_words)\n",
    "    \n",
    "    if common_words and len(common_words) / max(len(query_words), len(treatment_words)) > 0.6:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_about_bot(question):\n",
    "    \"\"\"Detect if the question is asking about the bot itself\"\"\"\n",
    "    question_lower = question.lower()\n",
    "    return any(keyword in question_lower for keyword in BOT_KEYWORDS)\n",
    "\n",
    "def get_bot_response():\n",
    "    \"\"\"Generate a professional response about the bot\"\"\"\n",
    "    return (f\"I am {BOT_INFO['name']}, a healthcare information assistant (version {BOT_INFO['version']}). \"\n",
    "            \"I provide evidence-based medical information and pricing details from my knowledge database. \"\n",
    "            \"While I can offer general healthcare information, I'm not a substitute for professional medical advice, \"\n",
    "            \"and my responses are limited to information in my database.\")\n",
    "\n",
    "def extract_price_info(docs):\n",
    "    \"\"\"Extract price information from document metadata and content\"\"\"\n",
    "    price_data = {}\n",
    "    for doc in docs:\n",
    "        # Check metadata if available\n",
    "        if doc.metadata.get('has_price_info') and 'price_data' in doc.metadata:\n",
    "            price_data.update(doc.metadata['price_data'])\n",
    "        # Also scan the content for price patterns\n",
    "        content = doc.page_content\n",
    "        price_matches = PRICE_PATTERN.findall(content)\n",
    "        for treatment, price in price_matches:\n",
    "            treatment = treatment.strip()\n",
    "            if treatment and not treatment.isdigit():\n",
    "                price_data[treatment.lower()] = price  # store keys in lowercase\n",
    "    return price_data\n",
    "\n",
    "def extract_price_from_text(text):\n",
    "    \"\"\"Extract treatment name and price from a text string.\"\"\"\n",
    "    match = TREATMENT_PRICE_PATTERN.search(text)\n",
    "    if match:\n",
    "        treatment = match.group(1).strip().lower()\n",
    "        price = match.group(2).strip()\n",
    "        return treatment, price\n",
    "    return None, None\n",
    "\n",
    "def format_retrieved_context(retrieved_docs):\n",
    "    \"\"\"Format documents with metadata and highlighted price information\"\"\"\n",
    "    price_info = extract_price_info(retrieved_docs)\n",
    "    context_parts = []\n",
    "    for doc in retrieved_docs:\n",
    "        content = doc.page_content\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        doc_type = f\" ({doc.metadata.get('document_type')})\" if doc.metadata.get('document_type') else \"\"\n",
    "        content_type = f\" - {doc.metadata.get('content_type')}\" if doc.metadata.get('content_type') else \"\"\n",
    "        context_parts.append(f\"• {content} (Source: {source}{doc_type}{content_type})\")\n",
    "    if price_info:\n",
    "        price_entries = [f\"• {treatment.title()}: {price} BDT\" for treatment, price in price_info.items()]\n",
    "        context_parts.append(\"\\nPrice Information:\\n\" + \"\\n\".join(price_entries))\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Cache frequently used prompts\n",
    "@functools.lru_cache(maxsize=16)\n",
    "def create_medical_prompt_template(include_price):\n",
    "    \"\"\"Create a template for medical prompts (cached)\"\"\"\n",
    "    price_instruction = \"Include price in BDT format if available. \" if include_price else \"\"\n",
    "    return f\"\"\"You are a professional medical assistant named MediAssist. Use ONLY the provided context to answer the question.\n",
    "If the information is not in the context, say \"I don't have information about that in my database. Please consult with a healthcare professional for personalized advice.\"\n",
    "\n",
    "IMPORTANT: Your response must be exactly 3-4 sentences total. Be concise, direct, and professional.\n",
    "\n",
    "{price_instruction}\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Response (3-4 sentences only, maintain professional tone):\"\"\"\n",
    "\n",
    "def create_medical_prompt(context, question, include_price):\n",
    "    \"\"\"Generate structured prompt with conditional pricing\"\"\"\n",
    "    template = create_medical_prompt_template(include_price)\n",
    "    return template.format(context=context, question=question)\n",
    "\n",
    "# Cache price prompt template\n",
    "@functools.lru_cache(maxsize=1)\n",
    "def create_price_prompt_template():\n",
    "    \"\"\"Create a template for price prompts (cached)\"\"\"\n",
    "    return \"\"\"You are MediAssist, a professional medical pricing assistant. Use ONLY the provided context to answer the question.\n",
    "If the pricing information is not in the context, say \"I don't have pricing information about that in my database. Please contact the relevant healthcare facility directly for current pricing.\"\n",
    "\n",
    "IMPORTANT: Your response must be exactly 3-4 sentences total. Be concise, direct, and professional.\n",
    "When mentioning prices, always use the format: X.00 BDT.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response (3-4 sentences only, maintain professional tone):\"\"\"\n",
    "\n",
    "def create_price_prompt(context, question):\n",
    "    \"\"\"Create a specialized prompt for price inquiries\"\"\"\n",
    "    template = create_price_prompt_template()\n",
    "    return template.format(context=context, question=question)\n",
    "\n",
    "def count_sentences(text):\n",
    "    \"\"\"Count the number of sentences in text\"\"\"\n",
    "    sentence_count = len(SENTENCE_PATTERN.findall(text))\n",
    "    return sentence_count\n",
    "\n",
    "def ensure_sentence_limit(text, max_sentences=4):\n",
    "    \"\"\"Ensure the text has no more than the specified number of sentences\"\"\"\n",
    "    # Simplified implementation for speed\n",
    "    sentences = re.split(r'([.!?])\\s+', text)\n",
    "    complete_sentences = []\n",
    "    i = 0\n",
    "    sentence_count = 0\n",
    "    while i < len(sentences) and sentence_count < max_sentences:\n",
    "        if i + 1 < len(sentences) and sentences[i+1] in ['.', '!', '?']:\n",
    "            complete_sentences.append(sentences[i] + sentences[i+1])\n",
    "            i += 2\n",
    "            sentence_count += 1\n",
    "        else:\n",
    "            if sentences[i] and sentences[i] not in ['.', '!', '?']:\n",
    "                if not sentences[i].strip().endswith(('.', '!', '?')):\n",
    "                    complete_sentences.append(sentences[i] + '.')\n",
    "                else:\n",
    "                    complete_sentences.append(sentences[i])\n",
    "                sentence_count += 1\n",
    "            i += 1\n",
    "    return ' '.join(complete_sentences)\n",
    "\n",
    "def format_price_response(response, price_info):\n",
    "    \"\"\"Format the response to include price information consistently\"\"\"\n",
    "    response = PRICE_FORMAT_PATTERN.sub(r'\\1.00 BDT', response)\n",
    "    response = SPACE_BEFORE_PRICE_PATTERN.sub(' ', response)\n",
    "    return response\n",
    "\n",
    "def process_direct_price_query(treatment_query):\n",
    "    \"\"\"Process queries that are direct price lookups\"\"\"\n",
    "    # This function handles logic for direct price lookups\n",
    "    # Use test index for treatment cost lookup with a higher k value\n",
    "    docs_with_scores = vector_store_test.similarity_search_with_score(treatment_query, k=20)\n",
    "    \n",
    "    # First attempt: Find match using our custom matching function\n",
    "    for doc, score in docs_with_scores:\n",
    "        treatment, price = extract_price_from_text(doc.page_content)\n",
    "        if treatment and treatments_match(treatment_query, treatment):\n",
    "            return f\"The cost for {treatment_query.title()} is {price}.00 BDT.\"\n",
    "    \n",
    "    # Second attempt: Extract any treatment name that looks like our query\n",
    "    potential_matches = []\n",
    "    for doc, score in docs_with_scores:\n",
    "        treatment, price = extract_price_from_text(doc.page_content)\n",
    "        if treatment and treatments_match(treatment_query, treatment):\n",
    "            potential_matches.append((treatment, price, score))\n",
    "    \n",
    "    # If we found potential matches, use the one with highest score\n",
    "    if potential_matches:\n",
    "        # Sort by score (lower is better)\n",
    "        potential_matches.sort(key=lambda x: x[2])\n",
    "        best_match = potential_matches[0]\n",
    "        return f\"The cost for {treatment_query.title()} is {best_match[1]}.00 BDT.\"\n",
    "    \n",
    "    # Third attempt: Look for price information in the text directly\n",
    "    for doc, score in docs_with_scores:\n",
    "        if treatment_query.lower() in doc.page_content.lower():\n",
    "            # Try to extract price using a more general pattern\n",
    "            price_match = GENERAL_PRICE_PATTERN.search(doc.page_content)\n",
    "            if price_match:\n",
    "                price = price_match.group(1)\n",
    "                return f\"The cost for {treatment_query.title()} is {price}.00 BDT.\"\n",
    "    \n",
    "    return \"I don't have pricing information about that in my database. Please contact the relevant healthcare facility directly for current pricing.\"\n",
    "\n",
    "def answer_question(question):\n",
    "    try:\n",
    "        # Check if this is a direct price query\n",
    "        m = BDT_QUERY_PATTERN.match(question)\n",
    "        if m:\n",
    "            treatment_query = m.group(1).strip()\n",
    "            return process_direct_price_query(treatment_query)\n",
    "        \n",
    "        # For other queries, use the medicalbot index\n",
    "        if is_about_bot(question):\n",
    "            return get_bot_response()\n",
    "        \n",
    "        # Detect if query is about pricing\n",
    "        question_lower = question.lower()\n",
    "        has_price = any(word in question_lower for word in PRICE_KEYWORDS)\n",
    "        \n",
    "        # Adjust search parameters based on query type\n",
    "        search_k = 5 if has_price else 3\n",
    "        similarity_threshold = 0.55 if has_price else 0.6\n",
    "        \n",
    "        # Enhance search query for price-related questions\n",
    "        search_query = question\n",
    "        if has_price and not any(term in search_query.lower() for term in ['price', 'cost']):\n",
    "            search_query += \" price cost\"\n",
    "            \n",
    "        # Perform vector search\n",
    "        docs_with_scores = vector_store_medicalbot.similarity_search_with_score(search_query, k=search_k)\n",
    "        filtered_docs = [doc for doc, score in docs_with_scores if score >= similarity_threshold]\n",
    "        \n",
    "        # Handle no results\n",
    "        if not filtered_docs:\n",
    "            return \"I don't have information about that in my database. Please consult with a healthcare professional for more specific guidance.\"\n",
    "        \n",
    "        # Extract price information if needed\n",
    "        price_info = {}\n",
    "        if has_price:\n",
    "            price_info = extract_price_info(filtered_docs)\n",
    "        \n",
    "        # Format context and create prompt\n",
    "        formatted_context = format_retrieved_context(filtered_docs)\n",
    "        if has_price:\n",
    "            prompt = create_price_prompt(formatted_context, question)\n",
    "        else:\n",
    "            prompt = create_medical_prompt(formatted_context, question, has_price)\n",
    "        \n",
    "        # Generate response with optimized settings\n",
    "        with llm.chat_session():\n",
    "            response = llm.generate(prompt, max_tokens=150, temp=0.5)\n",
    "        \n",
    "        # Post-process response\n",
    "        response = ensure_sentence_limit(response.strip())\n",
    "        if has_price:\n",
    "            response = format_price_response(response, price_info)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"I apologize, but I encountered a technical issue while processing your request. Please try again with a different question or contact support if the problem persists.\"\n",
    "\n",
    "# Use a request queue to handle concurrent requests better\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    question = data.get(\"question\", \"\")\n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"No question provided\"}), 400\n",
    "    \n",
    "    # Submit the question to the thread pool\n",
    "    future = executor.submit(answer_question, question)\n",
    "    answer = future.result()\n",
    "    \n",
    "    return jsonify({\"answer\": answer})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use production WSGI server in production\n",
    "    app.run(host=\"0.0.0.0\", port=8080, debug=False, threaded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final 5\n",
    "import os\n",
    "import re\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from gpt4all import GPT4All\n",
    "import functools\n",
    "import concurrent.futures\n",
    "from cachetools import TTLCache, LRUCache\n",
    "import time\n",
    "import random\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4NC22q_T6VQeA3w4cLszg6TQcFYpjQzUVWdqkHSN3oJbHJfw5QVkFdTLKg49oUYPQAfcTj\"\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"us-east-1\"\n",
    "\n",
    "# Initialize embeddings - load once at startup\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector stores - load once at startup\n",
    "vector_store_test = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"test\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "vector_store_medicalbot = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"medicalbot\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize LLM with optimized settings\n",
    "llm = GPT4All(\n",
    "    model_name=\"Qwen2-1.5B-Instruct.Q8_0.gguf\",\n",
    "    model_path=\"/Users/shakil/Downloads/RAG-Chatbot/model/\",\n",
    "    device=\"gpu\",\n",
    "    n_ctx=2048,  # Increased context window\n",
    "    n_threads=8,  # Adjust based on your CPU\n",
    "    n_batch=512   # Batch size for faster inference\n",
    ")\n",
    "\n",
    "# Pre-compile frequently used regex patterns\n",
    "PRICE_PATTERN = re.compile(r'([A-Za-z\\s\\-,/\\(\\)]+)\\s+(\\d+(?:\\.\\d+)?)\\s*(?:BDT|Tk\\.?)')\n",
    "TREATMENT_PRICE_PATTERN = re.compile(r'^\\d+\\s+([A-Za-z0-9\\s\\-,/\\(\\)]+?)\\s+BDT\\s+(\\d+(?:\\.\\d+)?)', flags=re.I)\n",
    "SENTENCE_PATTERN = re.compile(r'[.!?]+')\n",
    "PRICE_FORMAT_PATTERN = re.compile(r'(\\d+)(?:\\.00)?\\s*(?:BDT|Tk\\.?)')\n",
    "SPACE_BEFORE_PRICE_PATTERN = re.compile(r'(?<=\\S)(?=\\d+\\.00 BDT)')\n",
    "BDT_QUERY_PATTERN = re.compile(r'^\\s*(.+?)\\s*BDT\\s*$', flags=re.I)\n",
    "GENERAL_PRICE_PATTERN = re.compile(r'BDT\\s+(\\d+(?:\\.\\d+)?)')\n",
    "\n",
    "# Bot identity information\n",
    "BOT_INFO = {\n",
    "    \"name\": \"MediAssist\",\n",
    "    \"version\": \"1.1\",\n",
    "    \"description\": \"An advanced healthcare assistant providing accurate medical information and pricing.\",\n",
    "    \"capabilities\": [\n",
    "        \"Answering medical questions based on verified information\",\n",
    "        \"Providing treatment pricing information\",\n",
    "        \"Offering general healthcare guidance\",\n",
    "        \"Responding with natural conversational patterns\"\n",
    "    ],\n",
    "    \"limitations\": [\n",
    "        \"Cannot provide personalized medical diagnosis\",\n",
    "        \"Information is limited to the available knowledge database\",\n",
    "        \"Not a replacement for professional medical consultation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Precomputed bot keywords for faster matching\n",
    "BOT_KEYWORDS = frozenset([\n",
    "    \"who are you\", \"what are you\", \"your name\", \"about you\", \"tell me about yourself\",\n",
    "    \"what can you do\", \"your capabilities\", \"how do you work\", \"who made you\",\n",
    "    \"what is this\", \"chatbot\", \"medibot\", \"assistant\", \"your purpose\", \"your function\"\n",
    "])\n",
    "\n",
    "# Precomputed price keywords for faster matching\n",
    "PRICE_KEYWORDS = frozenset(['price', 'cost', 'how much', 'fee', 'charge', 'payment', 'rate', 'expense'])\n",
    "\n",
    "# Cache for storing responses to frequently asked questions (5 minute TTL)\n",
    "response_cache = TTLCache(maxsize=1000, ttl=300)\n",
    "\n",
    "# Cache for storing vector search results (10 minute TTL)\n",
    "vector_search_cache = TTLCache(maxsize=500, ttl=600)\n",
    "\n",
    "# Cache for storing commonly used treatment information\n",
    "treatment_cache = LRUCache(maxsize=200)\n",
    "\n",
    "# Human-like response variations\n",
    "RESPONSE_VARIATIONS = {\n",
    "    \"uncertainty\": [\n",
    "        \"I don't have information about that in my database. I'd recommend consulting with a healthcare professional for personalized advice.\",\n",
    "        \"That's beyond the scope of my current knowledge. Please consult with a qualified healthcare provider for specific guidance.\",\n",
    "        \"I don't have details on that in my records. It would be best to speak with a healthcare professional for accurate information.\",\n",
    "        \"I'm not able to provide information on that topic. I'd suggest reaching out to a medical professional for assistance.\"\n",
    "    ],\n",
    "    \"pricing_unknown\": [\n",
    "        \"I don't have pricing information about that in my database. Please contact the relevant healthcare facility directly for current pricing.\",\n",
    "        \"I don't have that specific pricing information available. For the most up-to-date rates, I'd recommend contacting the healthcare provider directly.\",\n",
    "        \"That pricing information isn't in my database. You'll need to check with the healthcare facility for their current rates.\",\n",
    "        \"I don't have access to that pricing data. For accurate cost information, please reach out to the healthcare provider directly.\"\n",
    "    ],\n",
    "    \"transitions\": [\n",
    "        \"Based on the information I have, \",\n",
    "        \"According to my database, \",\n",
    "        \"From what I understand, \",\n",
    "        \"The information I have indicates that \",\n",
    "        \"My records show that \",\n",
    "    ],\n",
    "    \"acknowledgments\": [\n",
    "        \"I see you're asking about \",\n",
    "        \"You're interested in \",\n",
    "        \"Regarding your question about \",\n",
    "        \"Concerning \",\n",
    "        \"About your inquiry on \",\n",
    "    ]\n",
    "}\n",
    "\n",
    "@functools.lru_cache(maxsize=256)\n",
    "def normalize_treatment_name(name):\n",
    "    \"\"\"Normalize treatment name for better matching (cached for performance)\"\"\"\n",
    "    return re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "\n",
    "@functools.lru_cache(maxsize=512)\n",
    "def treatments_match(query, treatment):\n",
    "    \"\"\"Check if treatment names match (cached for performance)\"\"\"\n",
    "    query_norm = normalize_treatment_name(query)\n",
    "    treatment_norm = normalize_treatment_name(treatment)\n",
    "    \n",
    "    # Direct match\n",
    "    if query_norm == treatment_norm:\n",
    "        return True\n",
    "    \n",
    "    # Check if one is a substring of the other\n",
    "    if query_norm in treatment_norm or treatment_norm in query_norm:\n",
    "        return True\n",
    "    \n",
    "    # Check if most words match\n",
    "    query_words = set(query_norm.split())\n",
    "    treatment_words = set(treatment_norm.split())\n",
    "    common_words = query_words.intersection(treatment_words)\n",
    "    \n",
    "    if common_words and len(common_words) / max(len(query_words), len(treatment_words)) > 0.6:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_about_bot(question):\n",
    "    \"\"\"Detect if the question is asking about the bot itself\"\"\"\n",
    "    question_lower = question.lower()\n",
    "    return any(keyword in question_lower for keyword in BOT_KEYWORDS)\n",
    "\n",
    "def get_bot_response():\n",
    "    \"\"\"Generate a professional response about the bot with human-like variations\"\"\"\n",
    "    variants = [\n",
    "        f\"I'm {BOT_INFO['name']}, a healthcare information assistant designed to provide evidence-based medical information and pricing details. While I offer general healthcare guidance, I'm not a substitute for professional medical advice.\",\n",
    "        \n",
    "        f\"I'm {BOT_INFO['name']}, an AI healthcare assistant that provides medical information and pricing details from verified sources. I can help with general healthcare questions, but remember that I'm not a replacement for consulting with a healthcare professional.\",\n",
    "        \n",
    "        f\"My name is {BOT_INFO['name']}, and I'm here to assist you with healthcare information and pricing inquiries. I draw from a database of verified medical knowledge, though I should mention that my responses aren't a substitute for professional medical consultation.\"\n",
    "    ]\n",
    "    \n",
    "    return random.choice(variants)\n",
    "\n",
    "def extract_price_info(docs):\n",
    "    \"\"\"Extract price information from document metadata and content\"\"\"\n",
    "    price_data = {}\n",
    "    for doc in docs:\n",
    "        # Check metadata if available\n",
    "        if doc.metadata.get('has_price_info') and 'price_data' in doc.metadata:\n",
    "            price_data.update(doc.metadata['price_data'])\n",
    "        # Also scan the content for price patterns\n",
    "        content = doc.page_content\n",
    "        price_matches = PRICE_PATTERN.findall(content)\n",
    "        for treatment, price in price_matches:\n",
    "            treatment = treatment.strip()\n",
    "            if treatment and not treatment.isdigit():\n",
    "                price_data[treatment.lower()] = price  # store keys in lowercase\n",
    "    return price_data\n",
    "\n",
    "def extract_price_from_text(text):\n",
    "    \"\"\"Extract treatment name and price from a text string.\"\"\"\n",
    "    match = TREATMENT_PRICE_PATTERN.search(text)\n",
    "    if match:\n",
    "        treatment = match.group(1).strip().lower()\n",
    "        price = match.group(2).strip()\n",
    "        return treatment, price\n",
    "    return None, None\n",
    "\n",
    "def format_retrieved_context(retrieved_docs):\n",
    "    \"\"\"Format documents with metadata and highlighted price information\"\"\"\n",
    "    price_info = extract_price_info(retrieved_docs)\n",
    "    context_parts = []\n",
    "    for doc in retrieved_docs:\n",
    "        content = doc.page_content\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        doc_type = f\" ({doc.metadata.get('document_type')})\" if doc.metadata.get('document_type') else \"\"\n",
    "        content_type = f\" - {doc.metadata.get('content_type')}\" if doc.metadata.get('content_type') else \"\"\n",
    "        context_parts.append(f\"- {content} (Source: {source}{doc_type}{content_type})\")\n",
    "    if price_info:\n",
    "        price_entries = [f\"- {treatment.title()}: {price} BDT\" for treatment, price in price_info.items()]\n",
    "        context_parts.append(\"\\nPrice Information:\\n\" + \"\\n\".join(price_entries))\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "@functools.lru_cache(maxsize=16)\n",
    "def create_medical_prompt_template(include_price):\n",
    "    \"\"\"Create a template for medical prompts (cached)\"\"\"\n",
    "    price_instruction = \"Include price in BDT format if available. \" if include_price else \"\"\n",
    "    return f\"\"\"You are a professional medical assistant named MediAssist. Use ONLY the provided context to answer the question.\n",
    "If the information is not in the context, say \"I don't have information about that in my database.\"\n",
    "\n",
    "IMPORTANT: Your response must be 3-5 sentences total. Be concise, direct, and professional but conversational.\n",
    "Use natural language with slight variations in sentence structure. Avoid repetitive patterns.\n",
    "Do not use hashtag symbols (#) in your responses.\n",
    "{price_instruction}\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Response (3-5 sentences only, maintain professional but conversational tone):\"\"\"\n",
    "\n",
    "def create_medical_prompt(context, question, include_price):\n",
    "    \"\"\"Generate structured prompt with conditional pricing\"\"\"\n",
    "    template = create_medical_prompt_template(include_price)\n",
    "    return template.format(context=context, question=question)\n",
    "\n",
    "@functools.lru_cache(maxsize=1)\n",
    "def create_price_prompt_template():\n",
    "    \"\"\"Create a template for price prompts (cached)\"\"\"\n",
    "    return \"\"\"You are MediAssist, a professional medical pricing assistant. Use ONLY the provided context to answer the question.\n",
    "If the pricing information is not in the context, say \"I don't have pricing information about that in my database.\"\n",
    "\n",
    "IMPORTANT: Your response must be 3-5 sentences total. Be concise, direct, and conversational.\n",
    "Use natural language with slight variations in sentence structure. Avoid repetitive patterns.\n",
    "Do not use hashtag symbols (#) in your responses.\n",
    "When mentioning prices, always use the format: X.00 BDT.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response (3-5 sentences only, maintain professional but conversational tone):\"\"\"\n",
    "\n",
    "def create_price_prompt(context, question):\n",
    "    \"\"\"Create a specialized prompt for price inquiries\"\"\"\n",
    "    template = create_price_prompt_template()\n",
    "    return template.format(context=context, question=question)\n",
    "\n",
    "def count_sentences(text):\n",
    "    \"\"\"Count the number of sentences in text\"\"\"\n",
    "    sentence_count = len(SENTENCE_PATTERN.findall(text))\n",
    "    return sentence_count\n",
    "\n",
    "def ensure_sentence_limit(text, max_sentences=5):\n",
    "    \"\"\"Ensure the text has no more than the specified number of sentences\"\"\"\n",
    "    sentences = re.split(r'([.!?])\\s+', text)\n",
    "    complete_sentences = []\n",
    "    i = 0\n",
    "    sentence_count = 0\n",
    "    while i < len(sentences) and sentence_count < max_sentences:\n",
    "        if i + 1 < len(sentences) and sentences[i+1] in ['.', '!', '?']:\n",
    "            complete_sentences.append(sentences[i] + sentences[i+1])\n",
    "            i += 2\n",
    "            sentence_count += 1\n",
    "        else:\n",
    "            if sentences[i] and sentences[i] not in ['.', '!', '?']:\n",
    "                if not sentences[i].strip().endswith(('.', '!', '?')):\n",
    "                    complete_sentences.append(sentences[i] + '.')\n",
    "                else:\n",
    "                    complete_sentences.append(sentences[i])\n",
    "                sentence_count += 1\n",
    "            i += 1\n",
    "    return ' '.join(complete_sentences)\n",
    "\n",
    "def format_price_response(response, price_info):\n",
    "    \"\"\"Format the response to include price information consistently\"\"\"\n",
    "    response = PRICE_FORMAT_PATTERN.sub(r'\\1.00 BDT', response)\n",
    "    response = SPACE_BEFORE_PRICE_PATTERN.sub(' ', response)\n",
    "    # Remove any remaining hashtags\n",
    "    response = response.replace('#', '')\n",
    "    return response\n",
    "\n",
    "def add_human_touch(response):\n",
    "    \"\"\"Add variations to make responses more human-like\"\"\"\n",
    "    # Remove any hashtags\n",
    "    response = response.replace('#', '')\n",
    "    \n",
    "    # Add transitions or acknowledgments at the beginning (30% chance)\n",
    "    if random.random() < 0.3 and not any(response.startswith(phrase) for phrase in RESPONSE_VARIATIONS[\"transitions\"] + RESPONSE_VARIATIONS[\"acknowledgments\"]):\n",
    "        response = random.choice(RESPONSE_VARIATIONS[\"transitions\"]) + response[0].lower() + response[1:]\n",
    "    \n",
    "    # Replace standard uncertainty phrases with variations\n",
    "    for phrase in [\"I don't have information about that in my database\"]:\n",
    "        if phrase in response:\n",
    "            response = response.replace(phrase, random.choice(RESPONSE_VARIATIONS[\"uncertainty\"]))\n",
    "    \n",
    "    # Replace standard pricing phrases with variations\n",
    "    for phrase in [\"I don't have pricing information about that in my database\"]:\n",
    "        if phrase in response:\n",
    "            response = response.replace(phrase, random.choice(RESPONSE_VARIATIONS[\"pricing_unknown\"]))\n",
    "    \n",
    "    return response\n",
    "\n",
    "def process_direct_price_query(treatment_query):\n",
    "    \"\"\"Process queries that are direct price lookups\"\"\"\n",
    "    # Check cache first\n",
    "    cache_key = f\"price_{normalize_treatment_name(treatment_query)}\"\n",
    "    if cache_key in treatment_cache:\n",
    "        return treatment_cache[cache_key]\n",
    "    \n",
    "    # Use test index for treatment cost lookup with a higher k value\n",
    "    search_key = f\"search_{normalize_treatment_name(treatment_query)}\"\n",
    "    if search_key in vector_search_cache:\n",
    "        docs_with_scores = vector_search_cache[search_key]\n",
    "    else:\n",
    "        docs_with_scores = vector_store_test.similarity_search_with_score(treatment_query, k=20)\n",
    "        vector_search_cache[search_key] = docs_with_scores\n",
    "    \n",
    "    # First attempt: Find match using our custom matching function\n",
    "    for doc, score in docs_with_scores:\n",
    "        treatment, price = extract_price_from_text(doc.page_content)\n",
    "        if treatment and treatments_match(treatment_query, treatment):\n",
    "            response = f\"The cost for {treatment_query.title()} is {price}.00 BDT. This is based on our current healthcare provider data. Remember that prices may vary slightly depending on the facility and any additional services required.\"\n",
    "            treatment_cache[cache_key] = response\n",
    "            return response\n",
    "    \n",
    "    # Second attempt: Extract any treatment name that looks like our query\n",
    "    potential_matches = []\n",
    "    for doc, score in docs_with_scores:\n",
    "        treatment, price = extract_price_from_text(doc.page_content)\n",
    "        if treatment and treatments_match(treatment_query, treatment):\n",
    "            potential_matches.append((treatment, price, score))\n",
    "    \n",
    "    # If we found potential matches, use the one with highest score\n",
    "    if potential_matches:\n",
    "        # Sort by score (lower is better)\n",
    "        potential_matches.sort(key=lambda x: x[2])\n",
    "        best_match = potential_matches[0]\n",
    "        response = f\"The cost for {treatment_query.title()} is {best_match[1]}.00 BDT. This pricing information is based on our current database. Please note that actual costs may vary depending on specific circumstances and the healthcare facility.\"\n",
    "        treatment_cache[cache_key] = response\n",
    "        return response\n",
    "    \n",
    "    # Third attempt: Look for price information in the text directly\n",
    "    for doc, score in docs_with_scores:\n",
    "        if treatment_query.lower() in doc.page_content.lower():\n",
    "            # Try to extract price using a more general pattern\n",
    "            price_match = GENERAL_PRICE_PATTERN.search(doc.page_content)\n",
    "            if price_match:\n",
    "                price = price_match.group(1)\n",
    "                response = f\"The cost for {treatment_query.title()} is {price}.00 BDT. This information is based on our records, but I'd recommend confirming with the healthcare provider for the most current pricing.\"\n",
    "                treatment_cache[cache_key] = response\n",
    "                return response\n",
    "    \n",
    "    response = random.choice(RESPONSE_VARIATIONS[\"pricing_unknown\"])\n",
    "    treatment_cache[cache_key] = response\n",
    "    return response\n",
    "\n",
    "def answer_question(question):\n",
    "    try:\n",
    "        # First check if we've seen this exact question before\n",
    "        question_key = question.strip().lower()\n",
    "        if question_key in response_cache:\n",
    "            return response_cache[question_key]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check if this is a direct price query\n",
    "        m = BDT_QUERY_PATTERN.match(question)\n",
    "        if m:\n",
    "            treatment_query = m.group(1).strip()\n",
    "            response = process_direct_price_query(treatment_query)\n",
    "            # Remove any hashtags\n",
    "            response = response.replace('#', '')\n",
    "            # Store in cache\n",
    "            response_cache[question_key] = response\n",
    "            return response\n",
    "        \n",
    "        # For other queries, use the medicalbot index\n",
    "        if is_about_bot(question):\n",
    "            response = get_bot_response()\n",
    "            response_cache[question_key] = response\n",
    "            return response\n",
    "        \n",
    "        # Detect if query is about pricing\n",
    "        question_lower = question.lower()\n",
    "        has_price = any(word in question_lower for word in PRICE_KEYWORDS)\n",
    "        \n",
    "        # Adjust search parameters based on query type\n",
    "        search_k = 5 if has_price else 3\n",
    "        similarity_threshold = 0.55 if has_price else 0.6\n",
    "        \n",
    "        # Enhance search query for price-related questions\n",
    "        search_query = question\n",
    "        if has_price and not any(term in search_query.lower() for term in ['price', 'cost']):\n",
    "            search_query += \" price cost\"\n",
    "        \n",
    "        # Check if we have this search cached\n",
    "        search_cache_key = f\"search_{search_query}\"\n",
    "        if search_cache_key in vector_search_cache:\n",
    "            docs_with_scores = vector_search_cache[search_cache_key]\n",
    "        else:\n",
    "            # Perform vector search\n",
    "            docs_with_scores = vector_store_medicalbot.similarity_search_with_score(search_query, k=search_k)\n",
    "            vector_search_cache[search_cache_key] = docs_with_scores\n",
    "            \n",
    "        filtered_docs = [doc for doc, score in docs_with_scores if score >= similarity_threshold]\n",
    "        \n",
    "        # Handle no results\n",
    "        if not filtered_docs:\n",
    "            response = random.choice(RESPONSE_VARIATIONS[\"uncertainty\"])\n",
    "            response_cache[question_key] = response\n",
    "            return response\n",
    "        \n",
    "        # Extract price information if needed\n",
    "        price_info = {}\n",
    "        if has_price:\n",
    "            price_info = extract_price_info(filtered_docs)\n",
    "        \n",
    "        # Format context and create prompt\n",
    "        formatted_context = format_retrieved_context(filtered_docs)\n",
    "        if has_price:\n",
    "            prompt = create_price_prompt(formatted_context, question)\n",
    "        else:\n",
    "            prompt = create_medical_prompt(formatted_context, question, has_price)\n",
    "        \n",
    "        # Generate response with optimized settings\n",
    "        with llm.chat_session():\n",
    "            response = llm.generate(\n",
    "                prompt, \n",
    "                max_tokens=200, \n",
    "                temp=0.7,  # Slightly higher temperature for more varied responses\n",
    "                repeat_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Post-process response\n",
    "        response = ensure_sentence_limit(response.strip(), 5)\n",
    "        if has_price:\n",
    "            response = format_price_response(response, price_info)\n",
    "        \n",
    "        # Add human-like variations and ensure no hashtags\n",
    "        response = add_human_touch(response)\n",
    "        response = response.replace('#', '')\n",
    "        \n",
    "        # Cache the response\n",
    "        response_cache[question_key] = response\n",
    "        \n",
    "        # Add thinking time for more natural feel (if response was too quick)\n",
    "        processing_time = time.time() - start_time\n",
    "        if processing_time < 0.5:\n",
    "            time.sleep(min(0.5, 0.5 - processing_time))\n",
    "            \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        # More human-like error responses\n",
    "        error_responses = [\n",
    "            \"I apologize, but I'm having trouble processing your request right now. Could you try rephrasing your question?\",\n",
    "            \"Something went wrong while I was retrieving that information. Can you try asking in a different way?\",\n",
    "            \"I seem to be having difficulty with that question. Could you try again or ask something else?\",\n",
    "            \"I ran into an issue while processing your question. Could you try again with a more specific query?\"\n",
    "        ]\n",
    "        return random.choice(error_responses)\n",
    "\n",
    "# Enhanced thread pool with priority queue\n",
    "class PriorityThreadPoolExecutor(concurrent.futures.ThreadPoolExecutor):\n",
    "    def __init__(self, max_workers=None):\n",
    "        super().__init__(max_workers=max_workers)\n",
    "        self.futures = {}\n",
    "        \n",
    "    def submit_with_priority(self, fn, *args, priority=0, **kwargs):\n",
    "        future = super().submit(fn, *args, **kwargs)\n",
    "        self.futures[future] = priority\n",
    "        return future\n",
    "\n",
    "# Use a request queue to handle concurrent requests better\n",
    "executor = PriorityThreadPoolExecutor(max_workers=8)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    question = data.get(\"question\", \"\")\n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"No question provided\"}), 400\n",
    "    \n",
    "    # Check if question is about pricing (higher priority)\n",
    "    has_price = any(word in question.lower() for word in PRICE_KEYWORDS)\n",
    "    priority = 1 if has_price else 0\n",
    "    \n",
    "    # Submit the question to the thread pool with priority\n",
    "    future = executor.submit_with_priority(answer_question, question, priority=priority)\n",
    "    answer = future.result()\n",
    "    \n",
    "    # Final check to ensure no hashtags in the response\n",
    "    answer = answer.replace('#', '')\n",
    "    \n",
    "    return jsonify({\"answer\": answer})\n",
    "\n",
    "@app.route(\"/health\", methods=[\"GET\"])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint for monitoring\"\"\"\n",
    "    return jsonify({\"status\": \"healthy\", \"model\": BOT_INFO[\"name\"], \"version\": BOT_INFO[\"version\"]})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use production WSGI server in production\n",
    "    app.run(host=\"0.0.0.0\", port=8080, debug=False, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
